{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine\n",
    "\n",
    "1. Connectors\n",
    "2. Indexation\n",
    "3. HashMap\n",
    "4. Save\n",
    "5. Query\n",
    "    * search(word)\n",
    "    * search word0 AND word1 AND word2\n",
    "    * search word0 OR word1 OR word2\n",
    "\n",
    "### Deps\n",
    "\n",
    "* Python3 (Python 3.7.3)\n",
    "    * os\n",
    "    * pickle\n",
    "\n",
    "### Parties\n",
    "\n",
    "#### Partie 1 : Obligatoire\n",
    "\n",
    "#### Partie 2 : Index Incrémental\n",
    "\n",
    "#### Partie 3 : Bonus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Mettre le path vers le dossier pour fetch\n",
    "path = \"./data/20news-bydate-train/alt.atheism\"\n",
    "\n",
    "path_politics = \"./data/20news-bydate-train/talk.politics.guns\"\n",
    "\n",
    "path_sci_space = \"./data/20news-bydate-train/sci.space\"\n",
    "path_sci_elect = \"./data/20news-bydate-train/sci.electronics/\"\n",
    "path_sci_med   = \"./data/20news-bydate-train/sci.med/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Obligatoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecteurs\n",
    "\n",
    "File system: prendre tous les fichier lisisbles d'un répertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "class Doc:\n",
    "    def __init__(self, url, text):\n",
    "        self.text = text # Raw strnig of the whole text  \n",
    "        self.url = url\n",
    "    pass\n",
    "\n",
    "\n",
    "def fetch(path, recursive=True):\n",
    "    DocList = []\n",
    "    \n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    dirs  = [d for d in listdir(path) if isdir(join(path, d))]\n",
    "    \n",
    "    for fname in files:\n",
    "        f = open(path + \"/\" + fname, \"r\", errors=\"ignore\")\n",
    "        try:\n",
    "            c = f.read()\n",
    "            DocList.append(Doc(path + \"/\" + fname, c))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        f.close\n",
    "    \n",
    "    if recursive:\n",
    "        for d in dirs:\n",
    "            DocList += fetch(join(path, d), recursive=recursive)              \n",
    "    \n",
    "    return DocList\n",
    "\n",
    "\n",
    "# res = fetch(\"/home/sidore_m/projects/search_engine/data/20news-bydate-train/\")\n",
    "\n",
    "DocList = fetch(path)\n",
    "\n",
    "# Debug\n",
    "print(len(DocList))\n",
    "# (DocList[0].url, DocList[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyseur\n",
    "\n",
    "* Transformer le texte brut des documents d'entrées en mots (effectuer des traitements sur ces mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.lower()\n",
    "        return proc_word\n",
    "    \n",
    "class AccentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\"'\", \" \")\n",
    "        proc_word = proc_word.replace(\"`\", \" \")\n",
    "        proc_word = proc_word.replace(\"\\\"\", \"\")\n",
    "        return proc_word\n",
    "    \n",
    "class PunctationProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\".\", \" \")\n",
    "        proc_word = proc_word.replace(\",\", \" \")\n",
    "        proc_word = proc_word.replace(\";\", \"\")\n",
    "        proc_word = proc_word.replace(\":\", \"\")\n",
    "        proc_word = proc_word.replace(\"!\", \"\")\n",
    "        proc_word = proc_word.replace(\"?\", \"\")\n",
    "        proc_word = proc_word.replace(\"\\n\", \" \")\n",
    "        return proc_word\n",
    "    \n",
    "class SpecialProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\"(\", \"\")\n",
    "        proc_word = proc_word.replace(\"-\", \" \")\n",
    "        proc_word = proc_word.replace(\"_\", \" \")\n",
    "        proc_word = proc_word.replace(\"=\", \" \")\n",
    "        proc_word = proc_word.replace(\"*\", \" \")\n",
    "        proc_word = proc_word.replace(\"<\", \"\")\n",
    "        proc_word = proc_word.replace(\">\", \"\")\n",
    "        proc_word = proc_word.replace(\")\", \"\")\n",
    "        proc_word = proc_word.replace(\"/\", \"\")\n",
    "        proc_word = proc_word.replace(\"\\\\\", \"\")\n",
    "        proc_word = proc_word.replace(\"[\", \"\")\n",
    "        proc_word = proc_word.replace(\"^\", \"\")\n",
    "        proc_word = proc_word.replace(\"]\", \"\")\n",
    "        proc_word = proc_word.replace(\"\", \"\")\n",
    "        proc_word = proc_word.replace(\" \", \"\")\n",
    "        proc_word = proc_word.replace(\"  \", \"\")\n",
    "        return proc_word     \n",
    "\n",
    "class TokenizedDoc:\n",
    "    def __init__(self, words, url):\n",
    "        self.words = words # list of strings (list of words)\n",
    "        self.url = url\n",
    "    pass    \n",
    "\n",
    "\n",
    "def analyse(docs, processorsList):\n",
    "    \"\"\"\n",
    "    docs           - list of Docs\n",
    "    processorsList - list of Processors like TextProcessor\n",
    "    return         - list of TokenizedDoc\n",
    "    \"\"\"\n",
    "    tokdocs = []\n",
    "    \n",
    "    for d in docs:\n",
    "        tokens = d.text.split(\" \") # Split and tokenzie on ' ', return list of words\n",
    "        # Process\n",
    "        words = []\n",
    "        for w in tokens:\n",
    "            w = w.strip()\n",
    "            for p in processorsList:\n",
    "                 w = p.process(w)\n",
    "            w_r = w.split()\n",
    "            for w in w_r:\n",
    "                if w != '' and w != ' ':        \n",
    "                    words.append(w)\n",
    "        tokdocs.append(TokenizedDoc(words, d.url))    \n",
    "    \n",
    "    return tokdocs\n",
    "    \n",
    "Processors = [PunctationProcessor, SpecialProcessor, AccentProcessor, LowerProcessor]\n",
    "                                      \n",
    "TokDocs = analyse(DocList, Processors)\n",
    "\n",
    "len(TokDocs)\n",
    "print(TokDocs[0].words[0], TokDocs[0].words[1], TokDocs[0].words[2],\n",
    "      TokDocs[1].words[2], TokDocs[2].words[30], TokDocs[2].words[45])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for KKK in TokDocs:\n",
    "#     print(KKK.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexeur\n",
    "\n",
    "* Transforme les listes de mots appartenant au document en des listes inversées de mots associés à des documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posting:\n",
    "    def __init__(self, word, urls):\n",
    "        \"\"\"\n",
    "        word - string\n",
    "        urls - list of url\n",
    "        \"\"\"\n",
    "        self.word = word\n",
    "        self.urls = urls\n",
    "    pass\n",
    "\n",
    "def make_index(TokeneizedDocs):\n",
    "    \"\"\"\n",
    "    TokeneizedDocs - list of TokenizedDoc\n",
    "    \"\"\"\n",
    "    done_words = []\n",
    "    postingList = []\n",
    "    \n",
    "    words = []\n",
    "    for tokD in TokeneizedDocs:\n",
    "        words += tokD.words\n",
    "        \n",
    "    print(\"Number of words \", len(words))    \n",
    "    \n",
    "        \n",
    "    for w in words:\n",
    "        if w in done_words:\n",
    "            continue\n",
    "                \n",
    "        done_words.append(w)\n",
    "        urls = []\n",
    "        for d in TokeneizedDocs:\n",
    "            if w in d.words:\n",
    "                urls.append(d.url)\n",
    "        postingList.append(Posting(w, urls))   \n",
    "    \n",
    "    return postingList\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, urlToDocId, wordToDocIds, idToUrl):\n",
    "        \"\"\"\n",
    "        urlToDocId   - Map/Dico <string, int> \n",
    "        wordToDocIds - Map/Dico <string, int[]> \n",
    "        idToUrl   - Map/Dico <int, string> \n",
    "        \"\"\"\n",
    "        self.urlToDocId = urlToDocId\n",
    "        self.idToUrl = idToUrl\n",
    "        self.wordToDocIds = wordToDocIds\n",
    "    pass\n",
    "\n",
    "\n",
    "def build(Postings):\n",
    "    \"\"\"\n",
    "    Build Index from list of Posting\n",
    "    \"\"\"\n",
    "    urlToId = {}\n",
    "    idToUrl = {}\n",
    "    wordToDocIds = {}\n",
    "    ids = 0\n",
    "    for i in Postings:\n",
    "        u_list = []\n",
    "        for j in i.urls:\n",
    "            if not (j in urlToId):\n",
    "                urlToId[j] = ids\n",
    "                idToUrl[ids] = j\n",
    "                u_list.append(ids)\n",
    "                ids += 1\n",
    "            else:\n",
    "                u_list.append(urlToId[j])\n",
    "        wordToDocIds[i.word] = u_list     \n",
    "        \n",
    "    return Index(urlToId, wordToDocIds, idToUrl)\n",
    "\n",
    "\n",
    "\n",
    "postingList = make_index(TokDocs)\n",
    "index = build(postingList)\n",
    "\n",
    "print(len(TokDocs), len(postingList))\n",
    "print(len(index.urlToDocId), len(index.wordToDocIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_bin_index = \"/home/sido/projects/SearchEngine/\"\n",
    "\n",
    "def save(index, path):\n",
    "    \"\"\"\n",
    "    Save index on disk\n",
    "    usin pickle python lib\n",
    "    \"\"\"\n",
    "    with open(path + \"/\" + \"index.bin\", \"wb\") as f:\n",
    "        pickle.dump(index, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save(index, path_bin_index)\n",
    "\n",
    "def load(path):\n",
    "    with open(path + \"/\" + \"index.bin\", \"rb\") as f:\n",
    "        index = pickle.load(f)\n",
    "    return index\n",
    "\n",
    "index = load(path_bin_index)\n",
    "print(len(index.urlToDocId), len(index.wordToDocIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searcher\n",
    "\n",
    "* Lire les listes inversées afin de répondre à une requete\n",
    "* Pouvoir lire un index sauvegardé précédemment sur disque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an object 'Index' as index should exist !\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(path + \"/\" + \"index.bin\", \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "            \n",
    "    \n",
    "    def search(self, word, nb_urls=50):\n",
    "        \"\"\"\n",
    "        Search word, return urls for this word\n",
    "        word      - string\n",
    "        nb_urls   - number of result urls to give back, if 0 return all\n",
    "        index_obj - specify an indexobject\n",
    "        \"\"\"\n",
    "        if not (word in self.index.wordToDocIds):\n",
    "            return \"No results, word not indexed\"\n",
    "        urls = self.index.wordToDocIds[word]\n",
    "        res = \"\"\n",
    "        for k in urls:\n",
    "            res += self.index.idToUrl[k] + \"\\n\"\n",
    "        return res\n",
    "\n",
    "\n",
    "    def searchAllOf(self, words):\n",
    "        \"\"\"\n",
    "        AND oper\n",
    "        words - list of words\n",
    "        \"\"\"\n",
    "        and_urls = []\n",
    "        first = True\n",
    "        for w in words:\n",
    "            if not (w in self.index.wordToDocIds):\n",
    "                return \"No results, one of the words is not indexed\"\n",
    "            urls = self.index.wordToDocIds[w]\n",
    "            if first:\n",
    "                and_urls = urls\n",
    "                first = False\n",
    "            else:\n",
    "                and_urls = list(set(and_urls) & set(urls))\n",
    "        res = \"\"\n",
    "        for k in and_urls:\n",
    "            res += self.index.idToUrl[k] + \"\\n\"\n",
    "        return res\n",
    "\n",
    "    def searchOneOf(self, words):\n",
    "        \"\"\"\n",
    "        OR oper\n",
    "        \"\"\"\n",
    "        or_urls = []\n",
    "        for w in words:\n",
    "            if not (w in self.index.wordToDocIds):\n",
    "                return \"No results, one of the words is not indexed\"\n",
    "            urls = self.index.wordToDocIds[w]\n",
    "            or_urls = list(set(or_urls) | set(urls))\n",
    "        res = \"\"\n",
    "        for k in or_urls:\n",
    "            res += self.index.idToUrl[k] + \"\\n\"\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = Searcher()\n",
    "searcher.load(path_bin_index)\n",
    "\n",
    "print(searcher.search('sports'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(searcher.searchAllOf(['sports', 'is']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(searcher.searchOneOf(['sports', 'and', 'of']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Index Incrémental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexation Incrementale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self, gen_id, wordToDocIds):\n",
    "        self.gen_id = gen_id\n",
    "        self.wordToDocIds = wordToDocIds\n",
    "\n",
    "class IndexIncr:\n",
    "    def __init__(self):\n",
    "        self.ids = 0\n",
    "        self.urlToDocId = {}\n",
    "        self.idToDocUrl = {}\n",
    "        self.docIdToGeneration = {}  # key (gen) = [doc id, ...]\n",
    "        self.generations = []\n",
    "        # Bonus 2 : Suppr vector\n",
    "        self.suppressions = []\n",
    "    \n",
    "    def build_gen(self, Postings):\n",
    "        \"\"\"\n",
    "        Build Index from list of Posting\n",
    "        \"\"\"\n",
    "        gen_id = len(self.generations)\n",
    "        wordToDocIds = {}\n",
    "        for i in Postings:\n",
    "            u_list = []\n",
    "            for j in i.urls:\n",
    "                if not (j in self.urlToDocId):\n",
    "                    self.urlToDocId[j] = self.ids\n",
    "                    self.idToDocUrl[self.ids] = j\n",
    "                    self.docIdToGeneration[self.ids] = gen_id\n",
    "                    u_list.append(self.ids)\n",
    "                    self.ids += 1\n",
    "                else:\n",
    "                    u_list.append(self.urlToDocId[j])\n",
    "            wordToDocIds[i.word] = u_list        \n",
    "        self.generations.append(Generation(gen_id, wordToDocIds))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index = IndexIncr()\n",
    "\n",
    "main_index.build_gen(postingList)\n",
    "\n",
    "DocList1 = fetch(path_politics)\n",
    "DocList2 = fetch(path_sci_space)\n",
    "\n",
    "TokDocs1 = analyse(DocList1, Processors)\n",
    "TokDocs2 = analyse(DocList2, Processors)\n",
    "\n",
    "postingList1 = make_index(TokDocs1)\n",
    "postingList2 = make_index(TokDocs2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index.build_gen(postingList1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index.build_gen(postingList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_bin_index = \".\"\n",
    "\n",
    "def save_incr(index, path):\n",
    "    \"\"\"\n",
    "    Save index on disk\n",
    "    usin pickle python lib\n",
    "    \"\"\"\n",
    "    with open(path + \"/\" + \"index_incr.bin\", \"wb\") as f:\n",
    "        pickle.dump(index, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "save_incr(main_index, path_bin_index)\n",
    "\n",
    "def load_incr(path):\n",
    "    with open(path + \"/\" + \"index_incr.bin\", \"rb\") as f:\n",
    "        index = pickle.load(f)\n",
    "    return index\n",
    "\n",
    "index_incr = load_incr(path_bin_index)\n",
    "index_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the SearchIncrIndex, I prefer retruning python list  as result,\n",
    "# and not like previously a string\n",
    "\n",
    "class SearchIncrIndex:\n",
    "    def __init__(self, index_incr=None):\n",
    "        if None == index_incr:\n",
    "            self.load(path_bin_index)\n",
    "        else:\n",
    "            self.index = index_incr\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(path + \"/\" + \"index_incr.bin\", \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "    \n",
    "    def id_to_url(self, id_l):\n",
    "        res = []\n",
    "        for i in id_l:\n",
    "            res.append(self.index.idToDocUrl[i])\n",
    "        return res\n",
    "    \n",
    "    def search(self, word):\n",
    "        res_id = self.search_ids(word)\n",
    "        res_urls = self.id_to_url(res_id)\n",
    "        return res_urls\n",
    "    \n",
    "    def search_ids(self, word):\n",
    "        \"\"\"\n",
    "        Search word, return urls for this word\n",
    "        word      - string\n",
    "        \"\"\"\n",
    "        gs = self.index.generations\n",
    "        res = []\n",
    "        gl = len(self.index.generations)\n",
    "        current_urls = []\n",
    "        for g in reversed(gs):  # Get last first\n",
    "            if not (word in g.wordToDocIds):\n",
    "                continue\n",
    "            preserved_urls = []\n",
    "            urls_ids = list.copy(g.wordToDocIds[word])\n",
    "            urls = []\n",
    "            for u in urls_ids:\n",
    "                urls.append(self.index.idToDocUrl[u])\n",
    "            for u in urls:\n",
    "                if not (u in current_urls):\n",
    "                    preserved_urls.append(u)\n",
    "                    current_urls.append(u)\n",
    "            urls_ids = []\n",
    "            for u in preserved_urls:\n",
    "                urls_ids.append(self.index.urlToDocId[u])\n",
    "            res = res + urls_ids\n",
    "        return res\n",
    "\n",
    "\n",
    "    def searchAllOf(self, words):\n",
    "        \"\"\"\n",
    "        AND oper\n",
    "        words - list of words\n",
    "        \"\"\"\n",
    "        and_urls = []\n",
    "        first = True\n",
    "        for w in words:\n",
    "            if first:\n",
    "                and_urls = self.search_ids(w)\n",
    "                first = False\n",
    "            else:\n",
    "                and_urls = list(set(and_urls) & set(self.search_ids(w)))\n",
    "        res_urls = self.id_to_url(and_urls)\n",
    "        return res_urls\n",
    "\n",
    "    def searchOneOf(self, words):\n",
    "        \"\"\"\n",
    "        OR oper\n",
    "        \"\"\"\n",
    "        or_urls = []\n",
    "        for w in words:\n",
    "            or_urls = list(set(or_urls) | set(self.search_ids(w)))\n",
    "        res_urls = self.id_to_url(or_urls)\n",
    "        return res_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher = SearchIncrIndex(index_incr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.search('sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['sports', 'is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchOneOf(['sports', 'and', 'of'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_incr.generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocList3 = fetch(path)\n",
    "\n",
    "TokDocs3 = analyse(DocList3, Processors)\n",
    "\n",
    "postingList3 = make_index(TokDocs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_incr.build_gen(postingList3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['sports', 'from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher = SearchIncrIndex(index_incr)\n",
    "incr_searcher.search('sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['sports', 'from'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['from', 'sido'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.searchAllOf(['from', 'weapons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Partie 3 : Bonus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 : MetaData\n",
    "### Bonus 2 : Suppression of Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, stat\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "\n",
    "class Doc:\n",
    "    def __init__(self, url, text, metadata={}):\n",
    "        self.text = text # Raw strnig of the whole text  \n",
    "        self.url = url\n",
    "        self.metadata = metadata\n",
    "    pass\n",
    "\n",
    "\n",
    "def fetch(path, recursive=True):\n",
    "    DocList = []\n",
    "    \n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    dirs  = [d for d in listdir(path) if isdir(join(path, d))]\n",
    "    \n",
    "    for fname in files:\n",
    "        f = open(path + \"/\" + fname, \"r\", errors=\"ignore\")\n",
    "        try:\n",
    "            c = f.read()\n",
    "            metadata = {}\n",
    "            metadata[\"name\"] = fname\n",
    "            metadata[\"file\"] = fname\n",
    "            metadata[\"date\"] = stat(path + \"/\" + fname).st_mtime\n",
    "            DocList.append(Doc(path + \"/\" + fname, c, metadata))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        f.close\n",
    "    \n",
    "    if recursive:\n",
    "        for d in dirs:\n",
    "            DocList += fetch(join(path, d), recursive=recursive)              \n",
    "    \n",
    "    return DocList\n",
    "\n",
    "\n",
    "# Changer le Path ici\n",
    "path = \"./data/20news-bydate-train/sci.space\"\n",
    "\n",
    "DocList = fetch(path)\n",
    "\n",
    "# Debug\n",
    "print(len(DocList))\n",
    "(DocList[0].url, DocList[0].text, DocList[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LowerProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.lower()\n",
    "        return proc_word\n",
    "    \n",
    "class AccentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\"'\", \" \")\n",
    "        proc_word = proc_word.replace(\"`\", \" \")\n",
    "        proc_word = proc_word.replace(\"\\\"\", \"\")\n",
    "        return proc_word\n",
    "    \n",
    "class PunctationProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\".\", \" \")\n",
    "        proc_word = proc_word.replace(\",\", \" \")\n",
    "        proc_word = proc_word.replace(\";\", \"\")\n",
    "        proc_word = proc_word.replace(\":\", \"\")\n",
    "        proc_word = proc_word.replace(\"!\", \"\")\n",
    "        proc_word = proc_word.replace(\"?\", \"\")\n",
    "        proc_word = proc_word.replace(\"\\n\", \" \")\n",
    "        return proc_word\n",
    "    \n",
    "class SpecialProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def process(word):\n",
    "        \"\"\"\n",
    "        word      - string\n",
    "        return    - processed string\n",
    "        \"\"\"\n",
    "        proc_word = word.replace(\"(\", \"\")\n",
    "        proc_word = proc_word.replace(\"-\", \" \")\n",
    "        proc_word = proc_word.replace(\"_\", \" \")\n",
    "        proc_word = proc_word.replace(\"=\", \" \")\n",
    "        proc_word = proc_word.replace(\"*\", \" \")\n",
    "        proc_word = proc_word.replace(\"<\", \"\")\n",
    "        proc_word = proc_word.replace(\">\", \"\")\n",
    "        proc_word = proc_word.replace(\")\", \"\")\n",
    "        proc_word = proc_word.replace(\"/\", \"\")\n",
    "        proc_word = proc_word.replace(\"\\\\\", \"\")\n",
    "        proc_word = proc_word.replace(\"[\", \"\")\n",
    "        proc_word = proc_word.replace(\"^\", \"\")\n",
    "        proc_word = proc_word.replace(\"]\", \"\")\n",
    "        proc_word = proc_word.replace(\"\", \"\")\n",
    "        proc_word = proc_word.replace(\" \", \"\")\n",
    "        proc_word = proc_word.replace(\"  \", \"\")\n",
    "        return proc_word     \n",
    "\n",
    "class TokenizedDoc:\n",
    "    def __init__(self, words, url):\n",
    "        self.words = words # list of strings (list of words)\n",
    "        self.url = url\n",
    "    pass    \n",
    "\n",
    "\n",
    "def analyse(docs, processorsList):\n",
    "    \"\"\"\n",
    "    docs           - list of Docs\n",
    "    processorsList - list of Processors like TextProcessor\n",
    "    return         - list of TokenizedDoc\n",
    "    \"\"\"\n",
    "    tokdocs = []\n",
    "    \n",
    "    for d in docs:\n",
    "        tokens = d.text.split(\" \") # Split and tokenzie on ' ', return list of words\n",
    "        # Process\n",
    "        words = []\n",
    "        for w in tokens:\n",
    "            w = w.strip()\n",
    "            for p in processorsList:\n",
    "                 w = p.process(w)\n",
    "            w_r = w.split()\n",
    "            for w in w_r:\n",
    "                if w != '' and w != ' ':        \n",
    "                    words.append(w)\n",
    "        tokdocs.append(TokenizedDoc(words, d.url))    \n",
    "    \n",
    "    return tokdocs\n",
    "    \n",
    "Processors = [PunctationProcessor, SpecialProcessor, AccentProcessor, LowerProcessor]\n",
    "                                      \n",
    "TokDocs = analyse(DocList, Processors)\n",
    "\n",
    "len(TokDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posting:\n",
    "    def __init__(self, word, urls):\n",
    "        \"\"\"\n",
    "        word - string\n",
    "        urls - list of url\n",
    "        \"\"\"\n",
    "        self.word = word\n",
    "        self.urls = urls\n",
    "    pass\n",
    "\n",
    "def make_index(TokeneizedDocs):\n",
    "    \"\"\"\n",
    "    TokeneizedDocs - list of TokenizedDoc\n",
    "    \"\"\"\n",
    "    done_words = []\n",
    "    postingList = []\n",
    "    \n",
    "    words = []\n",
    "    for tokD in TokeneizedDocs:\n",
    "        words += tokD.words\n",
    "        \n",
    "    print(\"Number of words \", len(words))     \n",
    "    for w in words:\n",
    "        if w in done_words:\n",
    "            continue\n",
    "                \n",
    "        done_words.append(w)\n",
    "        urls = []\n",
    "        for d in TokeneizedDocs:\n",
    "            if w in d.words:\n",
    "                urls.append(d.url)\n",
    "        postingList.append(Posting(w, urls))   \n",
    "    \n",
    "    return postingList\n",
    "\n",
    "\n",
    "postingList = make_index(TokDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generation:\n",
    "    def __init__(self, gen_id, wordToDocIds, didToMetaData):\n",
    "        self.gen_id = gen_id\n",
    "        self.wordToDocIds = wordToDocIds\n",
    "        # Bonus 1 : Meta Datas\n",
    "        self.didToMetaData = didToMetaData\n",
    "\n",
    "class IndexIncr:\n",
    "    def __init__(self):\n",
    "        self.ids = 0\n",
    "        self.urlToDocId = {}\n",
    "        self.idToDocUrl = {}\n",
    "        self.docIdToGeneration = {}  # key (gen) = [doc id, ...]\n",
    "        self.generations = []\n",
    "        # Bonus 2 : Suppr vector\n",
    "        self.suppressions = []\n",
    "    \n",
    "    def get_doc(self, doclist, url):\n",
    "        for i in doclist:\n",
    "            if i.url == url:\n",
    "                return i\n",
    "        return None\n",
    "    \n",
    "    def rm_doc(self, url):\n",
    "        \"\"\"\n",
    "        Remove a document\n",
    "        url - document url to remove\n",
    "        \"\"\"\n",
    "        url_id = self.urlToDocId[url]\n",
    "        if not (url_id in self.suppressions):\n",
    "            self.suppressions.append(url_id)\n",
    "        # print(self.suppressions)\n",
    "\n",
    "    \n",
    "    def build_gen(self, Postings, doclist):\n",
    "        \"\"\"\n",
    "        Build Index from list of Posting\n",
    "        \"\"\"\n",
    "        gen_id = len(self.generations)\n",
    "        wordToDocIds = {}\n",
    "        docIdToMetaData = {} # Dict of MetaDatas\n",
    "        for i in Postings:\n",
    "            u_list = []\n",
    "            for j in i.urls:\n",
    "                if not (j in self.urlToDocId):\n",
    "                    self.urlToDocId[j] = self.ids\n",
    "                    self.idToDocUrl[self.ids] = j\n",
    "                    self.docIdToGeneration[self.ids] = gen_id\n",
    "                    u_list.append(self.ids)\n",
    "                    x = self.get_doc(doclist, j)\n",
    "                    if x:\n",
    "                        docIdToMetaData[self.ids] = x.metadata\n",
    "                    else:\n",
    "                        docIdToMetaData[self.ids] = None\n",
    "                    self.ids += 1\n",
    "                else:\n",
    "                    u_list.append(self.urlToDocId[j])\n",
    "            wordToDocIds[i.word] = u_list        \n",
    "        self.generations.append(Generation(gen_id, wordToDocIds, docIdToMetaData))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index = IndexIncr()\n",
    "\n",
    "main_index.build_gen(postingList, DocList)\n",
    "\n",
    "\n",
    "DocList1 = fetch(path_politics)\n",
    "DocList2 = fetch(path_sci_space)\n",
    "DocList3 = fetch(path_sci_elect)\n",
    "DocList4 = fetch(path_sci_med)\n",
    "\n",
    "TokDocs1 = analyse(DocList1, Processors)\n",
    "TokDocs2 = analyse(DocList2, Processors)\n",
    "TokDocs3 = analyse(DocList3, Processors)\n",
    "TokDocs4 = analyse(DocList4, Processors)\n",
    "\n",
    "\n",
    "postingList1 = make_index(TokDocs1)\n",
    "postingList2 = make_index(TokDocs2)\n",
    "postingList3 = make_index(TokDocs3)\n",
    "postingList4 = make_index(TokDocs4)\n",
    "\n",
    "main_index.build_gen(postingList1, DocList1)\n",
    "main_index.build_gen(postingList2, DocList2)\n",
    "main_index.build_gen(postingList3, DocList3)\n",
    "main_index.build_gen(postingList4, DocList4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoundDocument:\n",
    "    def __init__(self, url, metaDatas):\n",
    "        self.url = url\n",
    "        self.metaDatas = metaDatas\n",
    "\n",
    "class SearchIncrIndex:\n",
    "    def __init__(self, index_incr=None):\n",
    "        if None == index_incr:\n",
    "            self.load(path_bin_index)\n",
    "        else:\n",
    "            self.index = index_incr\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(path + \"/\" + \"index_incr.bin\", \"rb\") as f:\n",
    "            self.index = pickle.load(f)\n",
    "    \n",
    "    def id_to_url(self, id_l):\n",
    "        res = []\n",
    "        for i in id_l:\n",
    "            res.append(self.index.idToDocUrl[i])\n",
    "        return res\n",
    "    \n",
    "    # Bonus 1 Search Metadatas, for 1 word search\n",
    "    def search_meta(self, word):\n",
    "        res = []\n",
    "        res_ids = self.search_ids(word)\n",
    "        for i in res_ids:\n",
    "            gen_id = self.index.docIdToGeneration[i] # Give gen of this doc id\n",
    "            gen = self.index.generations[gen_id]\n",
    "            url = self.index.idToDocUrl[i]\n",
    "            metas = gen.didToMetaData[i]\n",
    "            res.append(FoundDocument(url, metas))\n",
    "        return res\n",
    "    \n",
    "    def search(self, word):\n",
    "        res_id = self.search_ids(word)\n",
    "        res_urls = self.id_to_url(res_id)\n",
    "        return res_urls\n",
    "    \n",
    "    def search_ids(self, word):\n",
    "        \"\"\"\n",
    "        Search word, return urls for this word\n",
    "        word      - string\n",
    "        \"\"\"\n",
    "        gs = self.index.generations\n",
    "        res = []\n",
    "        gl = len(self.index.generations)\n",
    "        current_urls = []\n",
    "        for g in reversed(gs):  # Get last first\n",
    "            if not (word in g.wordToDocIds):\n",
    "                continue\n",
    "            preserved_urls = []\n",
    "            urls_ids = list.copy(g.wordToDocIds[word])\n",
    "            urls = []\n",
    "            for u in urls_ids:\n",
    "                urls.append(self.index.idToDocUrl[u])\n",
    "            for u in urls:\n",
    "                if not (u in current_urls):\n",
    "                    preserved_urls.append(u)\n",
    "                    current_urls.append(u)\n",
    "            urls_ids = []\n",
    "            for u in preserved_urls:\n",
    "                # Bonus 2 :  Handle supp\n",
    "                url_id = self.index.urlToDocId[u]\n",
    "                if url_id in self.index.suppressions:\n",
    "                    print(\"sup found\", self.index.suppressions)\n",
    "                    continue\n",
    "                urls_ids.append(url_id)\n",
    "            res = res + urls_ids\n",
    "        return res\n",
    "\n",
    "\n",
    "    def searchAllOf(self, words):\n",
    "        \"\"\"\n",
    "        AND oper\n",
    "        words - list of words\n",
    "        \"\"\"\n",
    "        and_urls = []\n",
    "        first = True\n",
    "        for w in words:\n",
    "            if first:\n",
    "                and_urls = self.search_ids(w)\n",
    "                first = False\n",
    "            else:\n",
    "                and_urls = list(set(and_urls) & set(self.search_ids(w)))\n",
    "        res_urls = self.id_to_url(and_urls)\n",
    "        return res_urls\n",
    "\n",
    "    def searchOneOf(self, words):\n",
    "        \"\"\"\n",
    "        OR oper\n",
    "        \"\"\"\n",
    "        or_urls = []\n",
    "        for w in words:\n",
    "            or_urls = list(set(or_urls) | set(self.search_ids(w)))\n",
    "        res_urls = self.id_to_url(or_urls)\n",
    "        return res_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher = SearchIncrIndex(main_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.search('space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = incr_searcher.search_meta('space')\n",
    "for r in res:\n",
    "    print(r.url, r.metaDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.search('weapon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher.search_meta('weapon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index.rm_doc('./data/20news-bydate-train/sci.space/60946')\n",
    "main_index.rm_doc('./data/20news-bydate-train/sci.space/61085')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_searcher = SearchIncrIndex(main_index)\n",
    "\n",
    "incr_searcher.search('weapon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_incr(main_index, path_bin_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
